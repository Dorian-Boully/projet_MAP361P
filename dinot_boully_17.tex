\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath, amssymb, amsthm, dsfont, IEEEtrantools}
\usepackage{enumitem}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\setlist[description]{
    labelindent=\parindent,
    leftmargin=\parindent,
    itemsep=\parskip,
}

\newcommand{\ens}[3][\middle|]{
    \left\{#2 \;\, #1 \; #3\right\}
}
\DeclareMathOperator{\Card}{\mathrm{Card}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\ind}{\mathds{1}}
\newcommand{\module}[1]{\left\lvert #1 \right\rvert}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\equi}[1]{\underset{#1}{\sim}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\theoremstyle{plain}
\newtheorem{thm}{Théorème}
\newtheorem{coro}{Corollaire}

\title{Estimation de la taille d'un graphe par marches aléatoires}
\author{Matthieu \textsc{Dinot}, Dorian \textsc{Boully}}
\date{}

\begin{document}
\maketitle
\section*{Préliminaire : spectre des matrices à coefficients positifs}
Pour une matrice réelle $X$ de taille quelconque (en particulier $X$ peut être un vecteur de $\R^{N}$ ou une matrice $(N,N)$), on note $X \geq 0$ lorsque tous ses coefficients sont positifs ou nuls, et $X > 0$ lorsque tous ses coefficients sont strictement positifs. On désigne par $X(i,j)$ le coefficient d'indice $(i,j)$ de $X$, et si $X$ est un vecteur de $\R^{N}$, on note $X(i)$ au lieu de $X(i,0)$.
\begin{thm}[Perron--Frobenius]\label{th:pf}
    Soit $A > 0 \in \mathcal{M}_N (\R)$. Alors
    \begin{itemize}
        \item $A$ possède une valeur propre réelle $\rho > 0$ telle que toutes les autres valeurs propres de $A$ sont de module strictement inférieur à $\rho$. On dit que $\rho$ est dominante
        \item Il existe un vecteur propre $X > 0$ associé à la valeur propre $\rho$.
        \item $\rho$ est une valeur propre simple de $A$, c'est à dire que sa multiplicité en tant que racine du polynôme caractéristique de $A$ vaut $1$. En particulier, le sous espace propre associé à $\rho$ est de dimension $1$.
    \end{itemize}
\end{thm}
Ce théorème nous est utile pour étudier le spectre des matrices stochastiques. Une matrice $A \in \mathcal{M}_N (\R)$ est dite stochastique lorsque $A \geq 0$ et 
$$\forall i \in \{1, \ldots, N\}, \quad \sum_{j = 1}^{N} A(i,j) = 1.$$ 
\begin{coro}\label{coro}
    Soit $A$ une matrice stochastique. On suppose qu'il existe un entier $k$ tel que $A^{k} > 0$ ($A$ est alors dite régulière). Alors $1$ est valeur propre simple et dominante de $A$.
\end{coro}

\begin{proof}[\emph{\textbf{Preuve du théorème \ref{th:pf}}}]
    Soit $\norm{\cdot}$ une norme sur $\R^{N}$. Posons $\mathcal{S} = \ens{X \in \R^N}{X \geq 0 \text{ et } \norm{X} = 1}$, qui est compact car fermé borné de $\R^N$, et $\Lambda = \ens{\lambda \in \R}{\exists X \in \mathcal{S}, \; (A - \lambda I)X \geq 0}$. On voit que $0 \in \Lambda$ et que $\Lambda$ est majoré par la somme des coefficients de $A$. Soit donc $\rho = \sup \Lambda \in \R$. Il existe des suites $(\lambda_n) \in \Lambda^{\N}$ et $(X_n) \in \mathcal{S}^{\N}$ telles que
    $$\lambda_n \xrightarrow[n \to + \infty]{} \rho \quad \text{et} \quad \forall n \in \N, \; (A - \lambda_n I) X_n \geq 0.$$
    $\mathcal{S}$ étant compact, on peut supposer quitte à extraire que $X_n$ converge vers $X \in \mathcal{S}$. En passant à la limite, il vient $(A - \rho I)X \geq 0$. Supposons que $(A - \rho I)X \neq 0$. Soit $Y = \frac{AX}{\norm{AX}} \in \mathcal{S}$. On a par construction $(A - \rho I)Y = \frac{1}{\norm{AX}}A(A - \rho I)X > 0$, et donc pour $\varepsilon > 0$ assez petit, $(A - (\rho + \varepsilon) I)Y > 0$, d'où $\rho + \varepsilon \in \Lambda$, ce qui est absurde. Ainsi, $AX = \rho X$. De plus $AX > 0$ car $A > 0$, $X \geq 0$ et $X \neq 0$, donc $\rho > 0$ et $X > 0$. $\rho$ est donc une valeur propre strictement positive de $A$ et il existe un vecteur propre $X > 0$ associé à $\rho$.

    Montrons que $\rho$ est dominante. Soit $\lambda \in \C$ une valeur propre de $A$ distincte de $\rho$, et $Z \in \C^N$ un vecteur propre associé à $\lambda$. On a
    $$\forall i, \quad \sum_j A(i,j)Z(j) = \lambda Z(i) \quad \text{donc} \quad \forall i, \quad  \module{\lambda}\module{Z(i)} \leq \sum_j A(i,j) \module{Z(j)}.$$ Ainsi, $\module{\lambda} \in \Lambda$, donc $\module{\lambda} \leq \rho$. Montrons que l'inégalité est stricte. Pour cela, on suppose par l'absurde que $\module{\lambda} = \module{\rho}$. En notant $\module{Z}$ le vecteur dont chaque composante est $\module{Z(i)}$, on a $(A - \module{\lambda}I)\module{Z} \geq 0$. Si $(A - \module{\lambda}I)\module{Z} \neq 0$, alors $(A - \module{\lambda}I)A\module{Z} > 0$ et par suite $\module{\lambda} < \rho$, ce qui est contraire aux hypothèses. Donc 
    $$\forall i, \quad \module{\lambda}\module{Z(i)} = \sum_j A(i,j) \module{Z(j)}.$$ Mais alors il existe $X \geq 0$, $X \neq 0$ et $\theta \in \R$ tels que $Z = e^{i\theta}X$. On en déduit que $AX = \lambda X$, et donc que $\lambda$ est réelle, et strictement positive. On a donc $\lambda = \module{\lambda} = \rho$, ce qui est absurde. $\rho$ est donc dominante.

    Montrons enfin que $\rho$ est simple. On commence par montrer que l'espace propre $E_{\rho}$ est de dimension $1$. Si ce n'est pas le cas, il existe deux vecteurs propres $X > 0$ et $Y \neq 0$ associés à $\rho$ tels que $(X,Y)$ est libre. Quitte à changer $Y$ en $-Y$, on peut supposer qu'une de ses composantes est strictement positive. Le réel $\alpha := \min\ens[:]{X(i)/Y(i)}{1 \leq i \leq N \text{ et } Y(i) > 0}$ est bien défini et vérifie $X - \alpha Y \geq 0$ mais $X - \alpha Y \ngtr 0$. Or, $X - \alpha Y$ est encore un vecteur propre de $A$ associé à $\rho$ (il est non nul par liberté de $(X, y)$), et $A > 0$ donc
    $$\rho (X - \alpha Y) = A(X - \alpha Y) > 0,$$
    ce qui est en contradiction avec $X - \alpha Y \ngtr 0$. Ainsi, $E_{\rho}$ est de dimension $1$. On améliore maintenant le résultat en montrant que $\rho$ est simple. Soit $(X_1 = X, \ldots, X_N)$ une base de $\R^{\N}$ qui complète un vecteur propre $X > 0$ associé à $\rho$. En notant $P$ la matrice dont la $i$-ième colonne est $X_i$, on a 
    $$P^{-1}AP = 
    \left(
    \begin{array}{c|ccc}
        \rho   & \star & \cdots & \star\\
         \hline
        0      &       &        &\\
        \vdots &       & B      &\\
        0      &       &        &
    \end{array}
    \right), \qquad \text{avec } B \in \mathcal{M}_{N - 1}(\R).$$
    Les polynômes caractéristiques $\chi_A$ et $\chi_B$ de $A$ et $B$ vérifient $\chi_A = (X - \rho) \chi_B$. Supposons que $\rho$ est racine d'ordre au moins $2$ de $\chi_A$. Alors $\rho$ est racine de $\chi_B$, donc valeur propre de $B$. Soit $Z \in \R^{N-1}$ un vecteur propre de $B$ associé à $\rho$. En posant $Y = P\begin{pmatrix} 0\\ Z \end{pmatrix}$, il vient
    $$AY = P 
    \left(
    \begin{array}{c|ccc}
        \rho   & \star & \cdots & \star\\
            \hline
        0      &       &        &\\
        \vdots &       & B      &\\
        0      &       &        &
    \end{array}
    \right)
    \begin{pmatrix} 0\\ Z \end{pmatrix}
    = P\begin{pmatrix}\alpha\\B Z\end{pmatrix}
    = P\begin{pmatrix}\alpha\\\rho Z\end{pmatrix}
    = \rho Y + \alpha X.$$
    Le réel $\alpha$ est non nul sinon $(X, Y)$ formerait une famille libre de vecteurs propres de $A$ associés à $\rho$. De plus, une récurrence immédiate donne $A^k Y = \rho^k Y + k\alpha \rho^{k-1}X$ pour $k \geq 0$, et donc
    $$A^k \module{Y} \geq \module{A^k Y} = \module{\rho^k Y + k\alpha \rho^{k-1}X} \geq \rho^{k-1}(k\module{\alpha} \module{X} - \rho \module{Y}) = \rho^{k-1}(k\module{\alpha} X - \rho \module{Y}).$$
    Comme $\alpha \neq 0$ et $X > 0$, on peut prendre $k$ suffisament grand pour que $k\alpha X - \rho \module{Y} > \rho Y$. On obtient $A^k \module{Y} > \rho^k \module{Y}$. Comme $A^k > 0$, on en déduit en appliquant le début de la preuve que $A^k$ possède une valeur propre strictement supérieure à $\rho ^k$. C'est absurde car toute valeur propre de $A^k$ est de la forme $\lambda^k$ avec $\lambda$ valeur propre de $A$ (par trigonalisation de $A$). On a donc montré que $\rho$ est racine d'ordre $1$ de $\chi_A$. Cela achève la preuve.
\end{proof}

\begin{proof}[\emph{\textbf{Preuve du corollaire \ref{coro}}}]
    Remarquons qu'une matrice $M$ est stochastique si et seulement si le veteur $U := (1, \ldots, 1)^{\top}$ vérifie $MU = U$. On en déduit facilement q'un produit de matrices stochastiques est stochastique. De plus, on a :
    $$\forall X \in \R^N, \forall i, \quad \module{(MX)(i)} = \module{\sum_j M(i,j)X(j)} \leq \sum_j M(i,j)\module{X(j)} \leq \sum_j M(i,j)\norm{X}_{\infty} = \norm{X}_{\infty}$$ donc $\norm{M} \leq 1$. Soit $k$ tel que $A^{k} > 0$. Comme $A^k > 0$, le théorème de Perron--Frobenius s'applique. Le fait que $\norm{A^k} \leq 1$ entraine $\rho \leq 1$, mais on a aussi $\rho \geq 1$ car $A^k U = U$. Donc $\rho = 1$. Ainsi, $1$ est valeur propre simple et dominante de $A^k$. Puis on trigonalise $A$. Il existe une matrice inversible $P$ telle que 
    $$A = P \left(
        \begin{array}{cccc}
            1      & \star     & \cdots & \star\\
            0      & \lambda_2 & \ddots & \vdots\\
            \vdots & \ddots    & \ddots & \star\\
            0      & \cdots    & 0      & \lambda_{N}
        \end{array}
    \right) P^{-1}
    \quad \text{ donc } \quad
    A^k = P \left(
        \begin{array}{cccc}
            1      & \star     & \cdots & \star\\
            0      & \lambda_{2}^{k} & \ddots & \vdots\\
            \vdots & \ddots    & \ddots & \star\\
            0      & \cdots    & 0      & \lambda_{N}^{k}
        \end{array}
    \right) P^{-1}.$$
    On en déduit que pour $2 \leq i \leq N$, $\module{\lambda_{i}}^{k} < 1$, donc $\module{\lambda_{i}} < 1$. Ainsi, $1$ est valeur propre dominante de $A$ et l'espace propre associé est de dimension $1$. En reprenant la fin de la preuve du théorème de Perron--Frobenius, on montre que $1$ est en fait simple. D'où le résultat.
\end{proof}

Il est possible d'appliquer ces résultats à l'étude d'une marche aléatoire sur un graphe $G = (V, E)$ de taille $N$ (orienté ou non). On peut définir une telle marche aléatoire à l'aide d'une matrice de transition $P$ telle que le coefficient $P(i,j)$ est la probabilité de passer du sommet $j$ au sommet $i$. On impose la contrainte $P(i,j) \neq 0$ si et seulement si $(j,i) \in E$. Ainsi $P$ représente bien une marche aléatoire sur le graphe $G$. On remarque que $P^{\top}$ est stochastique et que si $\pi_t$ représente le vecteur probabilité de présence à l'instant $t$, on a
$$\pi_{t+1} = P \pi_t.$$

Regardons maintenant sous quelles conditions il est possible d'appliquer le corollaire \ref{coro} à une matrice de transition $P$. Remarquons pour cela que $P^{k}(i,j)$ est non nul si et seulement si il existe un chemin de longueur $k$ de $j$ vers $i$ dans le graphe $G$. En effet, on a
$$P^{k}(i,j) = \sum_{l_1, \ldots, l_{k-1}} P(i,l_1)P(l_1, l_2)\cdots P(l_{k-1},j).$$
Chaque terme de cette somme est non nul lorsque $j \to l_{k-1} \to \cdots \to l_1 \to i$ est un chemin dans le graphe G. Comme $P \geq 0$, $P^{k}(i,j)$ est non nul si et seulement si au moins un terme de la somme est non nul, ce qui permet de conclure. Pour que $P$ soit régulière, il faut donc que $G$ soit connexe, mais cela ne suffit pas. Par exemple, si l'on prend un graphe biparti, les chemins de longueur paire ont un départ et une arrivée dans la même \og{}partie \fg{} du graphe, donc $P^{k}$ n'aura jamais tous ses coefficients non nuls. D'ailleurs, comme indiqué dans l'énoncé (\textbf{S1}), si $P$ représente une marche aléatoire sur un graphe biparti, où le choix du sommet au temps $t+1$ se fait uniformément parmi les voisins du sommet au temps $t$, alors $-1$ est dans le spectre de $P$. Par contre, une matrice de transition $P$ sur un graphe connexe tel que tout sommet est relié à lui même est régulière. En effet, comme le graphe est connexe, il existe un entier $k$ tel que pour tout $(i,j) \in V^2$, il existe un chemin de $j$ vers $i$ de longueur au plus $k$. En ajoutant autant de fois que nécessaire l'arête $(j,j)$ au début du chemin, on trouve un chemin de longueur exactement $k$. Justement, le fait de considérer des marches aléatoires paresseuses revient à ajouter toutes les arêtes $(j,j), \; j \in V$ au graphe étudié, ce qui ne change bien entendu pas sa taille et permet de garantir que $1$ est valeur propre dominante et simple. 
\section{Partie théorique}
Les questions suivantes restent valables dans le cas plus général d'une marche aléatoire sur un graphe $G$ non orienté connexe quelconque (pas forcément régulier) représentée par une matrice de transition (symétrique) $P$ dont $1$ est valeur propre simple et dominante.

\begin{description}
    \item[T1.\label{q:t1}] Soient $s \in \{0, \ldots, \tau - 1\}$ et $i \in V$. On a :
    \begin{align*}
    \P(X_{s+1}^{t} = i) & = \sum_{j \in V} \P((X_{s+1}^{t} = i) \cap (X_{s}^{t} = j))\\
    & = \sum_{j \in V}  \frac{1}{d}\ind_{E}(i,j) \P (X_{s}^{t} = j)\\
    & = \sum_{j \in V} P_{ij}\P (X_{s}^{t} = j).
    \end{align*}
    Ainsi $\left(\P(X_{s+1}^{t} = i)\right)_{i \in V} = P \cdot (X_{s}^{t} = i)_{i \in V}$. Une récurrence immédiate montre alors que
    \begin{equation}\label{eq:t1}
    \pi = P^{\tau} \cdot (\delta_{i,i_0})_{i \in V}
    \end{equation}
    où $\delta$ désigne le symbole de Krönecker.

    \item[T2.\label{q:t2}] Le théorème spectral assure l'existence d'une matrice orthogonale $O$ telle que
    \begin{equation*}\label{eq:diagonalisation}
    P = {}^{t}O \,\mathrm{Diag}(1,\lambda_2, \ldots, \lambda_N)\, O.
    \end{equation*}
     
    Vu les valeurs propres de $P$, on voit que, pour une norme quelconque sur les matrices $(N,N)$ (en particulier la norme subordonnée à $\left\lVert \cdot \right\rVert_2$) :
    
    \begin{equation*}\label{eq:limite_P}
    P^{\tau} = {}^{t}O\Delta^{\tau}O \xrightarrow[\tau \to +\infty]{} P^{\infty} := {}^{t}O \,\mathrm{Diag}(1,0, \ldots, 0)\,O
    \end{equation*}
    car la convergence a lieu coefficient par coefficient. La convergence de $(P^{\tau})_{\tau \geq 1}$ pour la norme subordonnée à $\norm{\cdot}_2$ entraine la convergence simple de la suite d'applications linéaires associées au sens de $\norm{\cdot}_2$ vers la même limite.
    On peut conclure de deux manières :
    \begin{itemize}
        \item Via un calcul explicite :
        \begin{equation*}
        \pi = P^{\tau}(\delta_{i,i_0})_{i \in V} \xrightarrow[\tau \to +\infty]{} P^{\infty}(\delta_{i,i_0})_{i \in V} = O_{1i_0} {}^{t}O \,^{t}(1, 0, \ldots, 0) = O_{1i_0} O^{-1} \,^{t}(1, 0, \ldots, 0).
        \end{equation*}
        Or $O^{-1} \,^{t}(1, 0, \ldots, 0)$ est le vecteur propre normalisé de $P$ de valeur propre $1$ (il y en a un seul car la valeur propre $1$ était simple). On vérifie facilement qu'il s'agit de $N^{-1/2} \,{}^{t}(1, \ldots, 1)$. Enfin, on a $O_{1i_0} = N^{-1/2}$ car c'est un coefficient de la première ligne de $O$, donc de la première colonne de $^{t}O = O^{-1}$, qui n'est autre que la matrice d'une base orthonormée de diagonalisation de $P$ dans la base canonique. On conclut comme attendu que 
        \begin{equation}\label{eq:pi_infini}
        \lim_{\tau \to +\infty} \pi = N^{-1}\,^{t}(1, \ldots, 1).
        \end{equation}
        \item En remarquant que $\pi^{\infty} := \displaystyle\lim_{\tau \to +\infty}\pi$ est le vecteur propre de $P$ associé à la valeur propre $1$ et dont la somme des coefficients vaut $1$. En effet, pour tout $\tau$, la somme des coefficients de $\pi$ vaut $1$, ce qui reste vrai à la limite (en particulier $\pi^{\infty} \neq 0$), et on a :
        $$P \pi^{\infty} = P\lim_{\tau \to +\infty} P^{\tau}(\delta_{i,i_0})_{i \in V} = \lim_{\tau \to +\infty} P^{\tau + 1}(\delta_{i,i_0})_{i \in V} = \pi^{\infty}.$$
        Encore une fois, on trouve la loi uniforme sur $V$.
    \end{itemize} 
    
    

    \item[T3.\label{q:t3}] Notons
    $$\mathcal{A}_m = \ens{(y_1,\ldots,y_m) \in V^{m}}{\Card \{y_1, \ldots, y_{m-1}\} =  \Card \{y_1, \ldots, y_{m}\} = m - (\ell - 1)},$$
    de sorte que
    $$(C_{\ell - 1} = m) = \bigsqcup_{\mathbf{y} \in \mathcal{A}_m} (\mathbf{Y} = \mathbf{y}).$$ 
    Notons aussi $\mathcal{B}_{n}(U)$ l'ensemble des $n$-uplets injectifs à valeurs dans $V \setminus U$. On a :
    \begin{align}
        \P((C_\ell - C_{\ell - 1} > n) \cap (C_{\ell - 1} = m)) & = \sum_{(y_1, \ldots, y_n) \in \mathcal{A}_m} \P\left((C_\ell - C_{\ell - 1} > n) \cap \bigcap_{1 \leq t \leq m}(Y_t = y_t)\right)\nonumber\\
        & = \sum_{\substack{
            (y_1, \ldots, y_m) \in \mathcal{A}_m\\
            (y_{m+1}, \ldots, y_{m+n}) \in \mathcal{B}_n(\{y_1, \ldots, y_m\})
            }}
            \P\left(\bigcap_{1 \leq t \leq m + n}(Y_t = y_t)\right)\nonumber\\
         &\label{eq:avant_lim_tau} = \sum_{\substack{
            (y_1, \ldots, y_m) \in \mathcal{A}_m\\
            (y_{m+1}, \ldots, y_{m+n}) \in \mathcal{B}_n(\{y_1, \ldots, y_m\})
            }}
            \prod_{1 \leq t \leq m+n}\P(Y_t = y_t)\\
         &\label{eq:apres_lim_tau} = \sum_{\substack{
            (y_1, \ldots, y_m) \in \mathcal{A}_m\\
            (y_{m+1}, \ldots, y_{m+n}) \in \mathcal{B}_n(\{y_1, \ldots, y_m\})
            }}
            \frac{1}{N^{m+n}}\\
        & =  \frac{1}{N^{m+n}} \sum_{(y_1, \ldots, y_m) \in \mathcal{A}_m} \Card \mathcal{B}_n(\{y_1, \ldots, y_m\})\nonumber\\
        & = \frac{1}{N^{m+n}} \sum_{(y_1, \ldots, y_m) \in \mathcal{A}_m} n! \binom{N - (m - (\ell - 1))}{n}\nonumber\\
        & = \frac{(N - m + \ell - 1)(N - m + \ell - 2) \cdots (N - m + \ell - n)}{N^n} \frac{\Card \mathcal{A}_m}{N^m}\nonumber\\
        & = \frac{(N - m + \ell - 1)(N - m + \ell - 2) \cdots (N - m + \ell - n)}{N^n} \P(C_{\ell - 1} = m).\nonumber
    \end{align}
    On trouve comme attendu :
    \begin{equation}\label{eq:t3}
        \P((C_\ell - C_{\ell - 1} > n) \mid (C_{\ell - 1} = m)) = \frac{(N - m + \ell - 1)(N - m + \ell - 2) \cdots (N - m + \ell - n)}{N^n}.
    \end{equation}
    On aurait pu démontrer ce fait de manière moins formelle, les idées essentielles étant que
    \begin{itemize}
        \item les $Y_t$ sont i.i.d. et suivent une loi uniforme sur $V$;
        \item le cardinal de $\mathcal{B}_{n}(U)$ ne dépend que de $n$ et du cardinal de $U$, mais pas des valeurs de ses éléments.
    \end{itemize}

    Si l'on ne fait pas l'approximation de remplacer les variables $Y_t$ par des variables uniformément distribuées sur $V$, il n'y a pas égalité entre les lignes (\ref{eq:avant_lim_tau}) et (\ref{eq:apres_lim_tau}). Cependant, la question \textbf{T2} montre que la ligne (\ref{eq:apres_lim_tau}) est la limite de la ligne (\ref{eq:avant_lim_tau}) lorsque $\tau$ tend vers l'infini. Ainsi, pour être tout à fait précis, on a montré que
    $$\lim_{\tau\to +\infty}\P((C_\ell - C_{\ell - 1} > n) \mid (C_{\ell - 1} = m)) = \frac{(N - m + \ell - 1)(N - m + \ell - 2) \cdots (N - m + \ell - n)}{N^n}.$$
    L'approximation est donc raisonnable jusqu'ici.

    \item[T4.] Nous allons légèrement améliorer le résultat de la première limite pour pouvoir en déduire la seconde. Soient $a, b$ des réels strictement positifs et $(a_N)_{N \geq 1}, (b_N)_{N \geq 1}$ des suites d'entiers telles que
    $$a_N \equi{+\infty} aN^{1/2} \quad \text{et} \quad b_N \equi{+\infty} bN^{1/2}.$$
    D'après la question précédente, on a :
    \begin{IEEEeqnarray*}{rCl}
        \P((C_\ell - C_{\ell - 1} > b_N) \mid (C_{\ell - 1} = a_{N})) & = & \frac{(N - a_{N} + \ell - 1)(N - a_{N} + \ell - 2) \cdots (N - a_{N} + \ell - b_{N})}{N^{b_{N}}}\\
        & = & \frac{(N - (a_{N} - (\ell - 1)))!}{N^{b_{N}} (N - b_{N} - (a_{N} - (\ell - 1))) !}\\
    \end{IEEEeqnarray*}
    Posons, pour $N \geq 1$
    \begin{IEEEeqnarray*}{rCl}
        u_N & = & N - (a_{N} - (\ell - 1))\\
        v_N & = & N - b_{N} - (a_{N} - (\ell - 1)).
    \end{IEEEeqnarray*}
    D'après la formule de Stirling, on a :
    \begin{IEEEeqnarray*}{rCl}
        \P((C_\ell - C_{\ell - 1} > b_N) \mid (C_{\ell - 1} = a_{N})) & \equi{+\infty} & \frac{\sqrt{2\pi u_N}}{\sqrt{2\pi v_N}} \frac{\exp \left[u_N \log u_N - u_N\right]}{\exp\left[b_{N} \log N + v_N \log v_N - v_N\right]}\\
        & \sim & \exp \left[u_N \log u_N + v_N - u_N - b_{N} \log N - v_N \log v_N\right].
    \end{IEEEeqnarray*} 
    On cherche un développement asymptotique en $o(1)$ de l'argument de l'exponentielle. On procède par étapes :
    \begin{IEEEeqnarray*}{rCl}
        \log u_N & = & \log N - \frac{a_{N} - (\ell - 1)}{N} - \frac{(a_{N} - (\ell - 1))^{2}}{2N^{2}} + o(1/N)\\
        & = & \log N - \frac{a_{N} - (\ell - 1)}{N} - \frac{(aN^{1/2} + o(N^{1/2}))^{2}}{2N^{2}} + o(1/N)\\
        & = & \log N - \frac{a_{N} - (\ell - 1)}{N} - \frac{a^{2}}{2N} + o(1/N).\\
    \end{IEEEeqnarray*}
    De même,
    \begin{IEEEeqnarray*}{rCl}
        \log v_N & = & \log N - \frac{b_{N} + a_{N} - (\ell - 1)}{N} - \frac{(b_{N} + a_{N} - (\ell - 1))^{2}}{2N^{2}} + o(1/N)\\
        & = & \log N - \frac{b_{N} + a_{N} - (\ell - 1)}{N} - \frac{(a+b)^{2}}{2N} + o(1/N).\\
    \end{IEEEeqnarray*}
    Puis
    \begin{IEEEeqnarray*}{rCl}
        \log u_N - \log v_N & = & \frac{b_{N}}{N} + \frac{b(2a+b)}{2N} + o(1/N).\\
    \end{IEEEeqnarray*}
    D'où, en utilisant le fait que $a_{N} = aN^{1/2} + o(N^{1/2})$ et $b_{N} = bN^{1/2} + o(N^{1/2})$
    \begin{IEEEeqnarray}{rCl}
        v_N (\log u_N - \log v_N) & = & b_{N} + \frac{b(2a+b)}{2} - \frac{b_{N} (b_{N} + a_{N})}{N} + o(1)\nonumber\\
        & = & b_{N} + \frac{b(2a+b)}{2} - b(a+b) + o(1)\nonumber\\
        \label{eq:a1}
        & = & b_{N} - \frac{b^{2}}{2} + o(1).
    \end{IEEEeqnarray}
    De plus, pour la même raison,
    \begin{IEEEeqnarray}{rCl}\label{eq:a2}
        b_{N} \log u_N & = & b_{N} \log N - ab + o(1).
    \end{IEEEeqnarray}
    Enfin, en utilisant les développements asymptotiques précédents (\ref{eq:a1} et \ref{eq:a2})
    \begin{IEEEeqnarray*}{rCl}
        u_N \log u_N + v_N - u_N - b_{N} \log N - v_N \log v_N & = & v_N (\log u_N - \log v_N) + b_{N} \log u_N - b_{N} - b_{N} \log N\\
        & = & b_{N} - \frac{b^{2}}{2} + b_{N} \log N - ab  - b_{N} - b_{N} \log N + o(1)\\
        & = & - ab - \frac{b^{2}}{2} + o(1).
    \end{IEEEeqnarray*}
    Cela montre que
    \begin{equation}\label{eq:t4a}
        \P((C_\ell - C_{\ell - 1} > b_N) \mid (C_{\ell - 1} = a_{N})) \xrightarrow[N \to +\infty]{} e^{-ab-b^{2}/2}.
    \end{equation}
    \'Etudions maintenant la suite 
    $$\P \left(\left(\frac{C_{\ell}^{2} - C_{\ell - 1}^{2}}{2N} > y\right) \middle| \left(\frac{C_{\ell - 1}^{2}}{2N} = \frac{\lfloor(2Nx)^{1/2}\rfloor^{2}}{2N}\right)\right)$$
    où $x, y > 0$. On écrit pour cela
    \begin{IEEEeqnarray*}{rCl}
        \IEEEeqnarraymulticol{3}{l}{
        \P\left(\left(\frac{C_{\ell}^{2} - C_{\ell - 1}^{2}}{2N} > y\right) \middle| \left(\frac{C_{\ell - 1}^{2}}{2N} = \frac{\lfloor(2Nx)^{1/2}\rfloor^{2}}{2N}\right)\right)
         }\\
        & = & \P\left(C_{\ell} > \left(\lfloor(2Nx)^{1/2}\rfloor^{2} + (2Ny)\right)^{1/2} \middle|C_{\ell - 1} = \lfloor(2Nx)\rfloor^{1/2}\right)\\
        & = & \P\left(C_{\ell} - C_{\ell - 1} > \left(\lfloor(2Nx)^{1/2}\rfloor^{2} + (2Ny)\right)^{1/2} - \lfloor(2Nx)^{1/2}\rfloor\middle|  C_{\ell - 1} = \lfloor(2Nx)^{1/2}\rfloor\right).\\
    \end{IEEEeqnarray*}
    Or,
    $$\lfloor(2Nx)^{1/2}\rfloor \equi{+\infty} (2Nx)^{1/2}$$
    et
    \begin{IEEEeqnarray*}{rCl}
        \left(\lfloor(2Nx)^{1/2}\rfloor^{2} + (2Ny)\right)^{1/2} - \lfloor(2Nx)^{1/2}\rfloor & = & \left(2N(x+y) + o(N)\right)^{1/2} - (2Nx)^{1/2} + o(N^{1/2})\\
        & = & (2N)^{1/2}\left[(x+y)^{1/2} - x^{1/2}\right] + o(N^{1/2}).
    \end{IEEEeqnarray*}
    On peut donc appliquer (\ref{eq:t4a}) avec $a = (2x)^{1/2}$ et $b = 2^{1/2}\left[(x+y)^{1/2} - x^{1/2}\right]$. On a 
    $$ab + b^{2}/2 = 2\left[x(x+y)\right]^{1/2} - 2x + (x+y) - 2\left[x(x+y)\right]^{1/2} + x = y,$$
    ce qui donne 
    \begin{equation}\label{eq:t4b}
        \P\left(\left(\frac{C_{\ell}^{2} - C_{\ell - 1}^{2}}{2N} > y\right) \middle| \left(\frac{C_{\ell - 1}^{2}}{2N} = \frac{\lfloor(2Nx)^{1/2}\rfloor^{2}}{2N}\right)\right) \xrightarrow[N \to +\infty]{} e^{-y}.
    \end{equation}
\end{description}

\end{document}